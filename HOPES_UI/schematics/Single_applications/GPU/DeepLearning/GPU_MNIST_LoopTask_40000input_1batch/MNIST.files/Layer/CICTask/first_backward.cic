/************************************
 *
 * File : f.cic
 * Date : Jul 24, 2018 11:12 AM
 *
*************************************/

/////////////////////////////////////
// include header section
/////////////////////////////////////

// ##DEFINE_SECTION::START
// ##DEFINE_SECTION::END
#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include <unistd.h>
#include <stdint.h>
#include <cublas_v2.h>

#define INPUT_DIMENSION 784
#define OUTPUT_DIMENSION 500

#define BLOCK 512

#define BATCH 1

const double ALPHA = 1;
const double BETA = 1;

static double cpuWeights[OUTPUT_DIMENSION][INPUT_DIMENSION];
static double *weights;

static double *inputAc;
static double *outputNet;

static double *delta;
static double *deltaWeight;
static double *error;

static double fanIn;
static double learningRate;
static double initWeight;

TASK_CODE_BEGIN

/////////////////////////////////////
// global definition
/////////////////////////////////////

// ##DEFINE_PORT_SECTION::START
STATIC int port_in_n;
STATIC int port_in_e;
STATIC int port_in_i;
STATIC int port_out_w;
// ##DEFINE_PORT_SECTION::END

/////////////////////////////////////
// init code
/////////////////////////////////////

static double getRandomGaussian()
{
	double v1, v2, s;
	do{
		v1 = 2.0f * ((double)rand()/RAND_MAX) - 1.0f;
		v2 = 2.0f * ((double)rand()/RAND_MAX) - 1.0f;
		s = v1*v1 + v2*v2;
	} while(s >= 1.0 || s == 0);

	s = sqrt( (-2.0 * log(s)) / s);
	return v1 * s;
}

static void initWeights()
{
    int i, j;
	for(i = 0; i < OUTPUT_DIMENSION; i++)
	{
		for(j = 0; j < INPUT_DIMENSION; j++)
		{
            cpuWeights[i][j] = getRandomGaussian() * initWeight * fanIn;
		}
	}
}

TASK_INIT
{
// ##INIT_PORT_SECTION::START
    port_in_n = PORT_INITIALIZE(TASK_ID, "in_output");
    port_in_e = PORT_INITIALIZE(TASK_ID, "in_error");
    port_in_i = PORT_INITIALIZE(TASK_ID, "in_prev_output");
    port_out_w = PORT_INITIALIZE(TASK_ID, "out_weight");
// ##INIT_PORT_SECTION::END

    // TODO: task initialize code
    cudaMalloc((void **) &weights, sizeof(double)*INPUT_DIMENSION * OUTPUT_DIMENSION);
    cudaMalloc((void **) &inputAc, sizeof(double)*INPUT_DIMENSION * BATCH);
    cudaMalloc((void **) &outputNet, sizeof(double)*OUTPUT_DIMENSION * BATCH);

    cudaMalloc((void **) &delta, sizeof(double)*OUTPUT_DIMENSION * BATCH);
    cudaMalloc((void **) &deltaWeight, sizeof(double)*OUTPUT_DIMENSION * BATCH);
    cudaMalloc((void **) &error, sizeof(double)*INPUT_DIMENSION * BATCH);
    
    fanIn = sqrt(1.0 / INPUT_DIMENSION);
    learningRate = 0.3 * fanIn;
    initWeight = 0.2;
    
    initWeights();
    cudaMemcpy(weights, cpuWeights, sizeof(double) * OUTPUT_DIMENSION * INPUT_DIMENSION, cudaMemcpyHostToDevice);
    
    MQ_SEND(port_out_w, (unsigned char*)weights, sizeof(double) * OUTPUT_DIMENSION * INPUT_DIMENSION);
}


/////////////////////////////////////
// go code
/////////////////////////////////////
static int cuda_get_device() {
    int n = 0;
    cudaError_t status = cudaGetDevice(&n);
    return n;
}

static cublasHandle_t blas_handle() {
    static int init[16] = {0};
    static cublasHandle_t handle[16];
    int i = cuda_get_device();
    if (!init[i]) {
        cublasCreate(&handle[i]);
        init[i] = 1;
    }
    return handle[i];
}

static dim3 cuda_gridsize(size_t n) {
    size_t k = (n - 1) / BLOCK + 1;
    size_t x = k;
    size_t y = 1;
    if (x > 65535) {
        x = ceil(sqrt(k));
        y = (n - 1) / (x * BLOCK) + 1;
    }
    dim3 d3(x, y, 1);
    return d3;
}

__device__ static double _derivate_relu_gpu(double x) {
    return (x > 0 ? 1 : 0);
}

__global__ static void derivate_relu_kernel(double *x, double *y, double *z, int n) {
    int i = (blockIdx.x + blockIdx.y * gridDim.x) * blockDim.x + threadIdx.x;
    if (i < n) {
        x[i] = _derivate_relu_gpu(y[i]) * z[i];
    }
}

static void calcDeltaWeight(double *x, double *y, double *z, int n) {
    derivate_relu_kernel<<<cuda_gridsize(n), BLOCK>>>(x, y, z, n);
}

static double neuron_Derivative(double x){
    if (x>0){
        return 1;
    }
    else {
        return 0;
    }
}

TASK_GO
{
    // TODO: task main code
    MQ_RECEIVE(port_in_n, (unsigned char*)outputNet, sizeof(double) * OUTPUT_DIMENSION * BATCH);
    MQ_RECEIVE(port_in_i, (unsigned char*)inputAc, sizeof(double) * INPUT_DIMENSION * BATCH);
    MQ_RECEIVE(port_in_e, (unsigned char*)delta, sizeof(double) * OUTPUT_DIMENSION * BATCH);

    cudaMemset(error, 0, sizeof(double) * INPUT_DIMENSION * BATCH);

    calcDeltaWeight(deltaWeight, outputNet, delta, OUTPUT_DIMENSION * BATCH);
    
    cublasHandle_t handle = blas_handle();
    cublasStatus_t status1 = cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, INPUT_DIMENSION, BATCH, OUTPUT_DIMENSION,
        &ALPHA, weights, INPUT_DIMENSION, deltaWeight, OUTPUT_DIMENSION, &BETA, error, INPUT_DIMENSION);

    cublasHandle_t handle2 = blas_handle();
    cublasStatus_t status2 = cublasDgemm(handle2, CUBLAS_OP_N, CUBLAS_OP_T, INPUT_DIMENSION, OUTPUT_DIMENSION, BATCH,
        &learningRate, inputAc, INPUT_DIMENSION, deltaWeight, OUTPUT_DIMENSION, &BETA, weights, INPUT_DIMENSION);
	
	MQ_SEND(port_out_w, (unsigned char*)weights, sizeof(double) * OUTPUT_DIMENSION * INPUT_DIMENSION);
}



/////////////////////////////////////
// wrapup code
/////////////////////////////////////

TASK_WRAPUP
{
    // TODO: task wrapup code
    cudaFree(weights);
    cudaFree(inputAc);
    cudaFree(outputNet);
    
    cudaFree(delta);
    cudaFree(deltaWeight);
    cudaFree(error);
}

TASK_CODE_END