/************************************
 *
 * File : f.cic
 * Date : Oct 30, 2018 3:24 PM
 *
*************************************/

/////////////////////////////////////
// include header section
/////////////////////////////////////

#include <stdio.h>

#include "activate.h"
#include "blas.h"
#include "cuda.h"
#include "weight.h"

// ##DEFINE_SECTION::START
#define BATCH 1

#define KERNEL_WIDTH    1
#define KERNEL_HEIGHT   1
#define STRIDE          1
#define PADDING_WIDTH   0
#define PADDING_HEIGHT  0
#define GROUPS          1

#define NUMBER_OF_LAYERS 152

#define EXECUTION_COUNT 36

#define PRINTopt 0
// ##DEFINE_SECTION::END

static int IN_WIDTH = 14;
static int IN_HEIGHT = 14;
static int IN_CHANNEL = 1024;
    
static int OUT_WIDTH = 14;
static int OUT_HEIGHT = 14;
static int OUT_CHANNEL = 256;

static float *pfData;
static float *pfWeight;
static float *pfBias;

static float *pfInputGPU;
static float *pfOutputGPU;
static float *pfWorkspace;
static float *pfWeightGPU[EXECUTION_COUNT];
static float *pfBiasGPU[EXECUTION_COUNT];

static float *scales;
static float *rolling_mean;
static float *rolling_variance;

static cudnnTensorDescriptor_t srcTensorDesc;
static cudnnTensorDescriptor_t dstTensorDesc;
static cudnnTensorDescriptor_t biasTensorDesc;

static cudnnFilterDescriptor_t weightDesc;
static cudnnConvolutionDescriptor_t convDesc;
static cudnnConvolutionFwdAlgo_t fw_algo;
static cudnnConvolutionFwdAlgo_t fw_algo_512;
static cudnnConvolutionFwdAlgo_t fw_algo_1024;

static size_t workspace_size;
static size_t workspace_maxsize;

static float *weight;
static float *pfInitWeight;

static int nLayer[EXECUTION_COUNT] = {35, 38, 41, 44, 47, 50,
                                      53, 56, 59, 62, 65, 68,
                                      71, 74, 77, 80, 83, 86,
                                      89, 92, 95, 98, 101, 104,
                                      107, 110, 113, 116, 119, 122,
                                      125, 128, 131, 134, 137, 140};

TASK_CODE_BEGIN

/////////////////////////////////////
// global definition
/////////////////////////////////////

// ##DEFINE_PORT_SECTION::START
STATIC int port_input;
STATIC int port_output;
// ##DEFINE_PORT_SECTION::END

/////////////////////////////////////
// init code
/////////////////////////////////////

static cudaStream_t stream;


static void nobn_weight_do(int idx, int layer_idx)
{
    int nWeightSize = IN_CHANNEL/GROUPS * KERNEL_WIDTH * KERNEL_HEIGHT * OUT_CHANNEL;
    int filter, size;
    int conv_size = IN_CHANNEL/GROUPS * KERNEL_WIDTH * KERNEL_HEIGHT;

    for(filter = 0; filter < OUT_CHANNEL; filter++){
        float variance = rolling_variance[filter] + .00001f;
        float invstd = scales[filter] / sqrt(variance);
        pfBias[filter] =  pfBias[filter] - (rolling_mean[filter]) * invstd;
        for(size = 0; size < conv_size; size++){
            pfWeight[filter*conv_size + size] =  pfWeight[filter*conv_size + size] * invstd;
            if(isnan(pfWeight[filter*conv_size + size]))
                printf("idx : %d filter : %d size : %d pfWeight : %f\n",idx, filter, size, pfWeight[filter*conv_size + size]);
        }
        if(isnan(pfBias[filter])){
            printf("idx : %d filter : %d pfBias : %f\n",idx, filter, pfBias[filter]);
        }  
    }
    cuda_push_array(pfWeightGPU[idx], pfWeight, nWeightSize);
    cuda_push_array(pfBiasGPU[idx], pfBias, OUT_CHANNEL);
}


static void cudnn_convolutional_setup() {
    cudnnSetTensor4dDescriptor(srcTensorDesc, CUDNN_TENSOR_NCHW,
                               CUDNN_DATA_FLOAT, BATCH, IN_CHANNEL, IN_HEIGHT,
                               IN_WIDTH);
    cudnnSetTensor4dDescriptor(dstTensorDesc, CUDNN_TENSOR_NCHW,
                               CUDNN_DATA_FLOAT, BATCH, OUT_CHANNEL, OUT_HEIGHT,
                               OUT_WIDTH);
    cudnnSetTensor4dDescriptor(biasTensorDesc, CUDNN_TENSOR_NCHW,
                               CUDNN_DATA_FLOAT, 1, OUT_CHANNEL, 1,
                               1); 
    cudnnSetFilter4dDescriptor(weightDesc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW,
                               OUT_CHANNEL, IN_CHANNEL/GROUPS, KERNEL_HEIGHT,
                               KERNEL_WIDTH);
#if CUDNN_MAJOR >= 6
    cudnnSetConvolution2dDescriptor(convDesc, PADDING_HEIGHT, PADDING_WIDTH, STRIDE, STRIDE, 1, 1,
                                    CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT);
#else
    cudnnSetConvolution2dDescriptor(convDesc, PADDING_HEIGHT, PADDING_WIDTH, STRIDE, STRIDE, 1, 1,
                                    CUDNN_CROSS_CORRELATION);
#endif
#if CUDNN_MAJOR >= 7
    cudnnSetConvolutionGroupCount(convDesc, GROUPS);
#else
    if (GROUPS > 1) {
        perror("CUDNN < 7 doesn't support groups, please upgrade!");
        exit(-1);
    }   
#endif
}

static cudnnConvolutionFwdAlgo_t getForwardAlgorithm()
{
    int requested_algo_count = 0, returned_algo_count = 0;
    int found_conv_algorithm = 0;
	float min_time = 1000000;   // 1000 sec
	cudnnConvolutionFwdAlgo_t fw_get;
	cudnnConvolutionFwdAlgoPerf_t conv_fwd_results[100];
	cudnnSetConvolutionMathType(convDesc, CUDNN_TENSOR_OP_MATH);

	cudnnGetConvolutionForwardAlgorithmMaxCount(cudnn_handle(), &requested_algo_count);

    cudnnGetConvolutionForwardAlgorithm_v7(
        cudnn_handle(), srcTensorDesc, weightDesc, convDesc, dstTensorDesc,
        requested_algo_count, &returned_algo_count, conv_fwd_results);

	fw_get = conv_fwd_results[0].algo;
    for (int i = 0; i < returned_algo_count; i++)
    {
		//printf(" - cuDNN FWD algo: %d, time = %f ms \n", conv_fwd_results[i].algo, conv_fwd_results[i].time);
        if (conv_fwd_results[i].status == CUDNN_STATUS_SUCCESS &&
//            conv_fwd_results[i].algo != CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED &&
          conv_fwd_results[i].time > 0 && conv_fwd_results[i].time < min_time)
        {
            fw_get = conv_fwd_results[i].algo;
            min_time = conv_fwd_results[i].time;
            //printf(" - cuDNN FWD algo: %d, time = %f ms \n", l->fw_algo, min_time);
        }
    }	

	return fw_get;
}


TASK_INIT
{
    if(PRINTopt == 1)
        printf("333thirdblock_first_conv) init start\n");
// ##INIT_PORT_SECTION::START
    port_input = PORT_INITIALIZE(TASK_ID, "input");
    port_output = PORT_INITIALIZE(TASK_ID, "output");
// ##INIT_PORT_SECTION::END

    // TODO: task initialize code
    int i;
    float *pfTemp;

    int nWeightSize, nInSize, nOutSize, nMaxInSize, nMaxOutSize;
    int layer_idx;

    cudnnGetStream(cudnn_handle(), &stream);
    
    nMaxInSize = 28 * 28 * 512;
    nMaxOutSize = 28 * 28 * 256;

    pfInputGPU = cuda_make_array((float *)NULL, BATCH * nMaxInSize);
    pfOutputGPU = cuda_make_array((float *)NULL, BATCH * nMaxOutSize);


    for(i = 0; i < EXECUTION_COUNT; i++){
        layer_idx = nLayer[i];
        
        if(layer_idx == 35){
            IN_WIDTH = 28; IN_HEIGHT = 28; IN_CHANNEL = 512; 
            OUT_WIDTH = 28; OUT_HEIGHT = 28; OUT_CHANNEL = 256; 
        }
        else{
            IN_WIDTH = 14; IN_HEIGHT = 14; IN_CHANNEL = 1024;
            OUT_WIDTH = 14; OUT_HEIGHT = 14; OUT_CHANNEL = 256;
        }
        nWeightSize = IN_CHANNEL/GROUPS * KERNEL_WIDTH * KERNEL_HEIGHT * OUT_CHANNEL;
        pfInitWeight = (float *)malloc((nWeightSize + OUT_CHANNEL * 4) * sizeof(float));
        get_weight(layer_idx, pfInitWeight, nWeightSize + OUT_CHANNEL * 4);
        pfTemp = pfInitWeight;
        
        // data to be saved
        scales = pfInitWeight + OUT_CHANNEL;
        rolling_mean = scales + OUT_CHANNEL;
        rolling_variance = rolling_mean + OUT_CHANNEL;
        pfTemp = rolling_variance;
        
        nWeightSize = IN_CHANNEL/GROUPS * KERNEL_WIDTH * KERNEL_HEIGHT * OUT_CHANNEL;
        nInSize = IN_WIDTH * IN_HEIGHT * IN_CHANNEL;
        nOutSize = OUT_WIDTH * OUT_HEIGHT * OUT_CHANNEL;
        
        pfWeight = pfTemp + OUT_CHANNEL;
        pfBias = pfInitWeight;
        
        pfWeightGPU[i] = cuda_make_array(pfWeight, nWeightSize);
        pfBiasGPU[i] = cuda_make_array(pfInitWeight, OUT_CHANNEL);
        nobn_weight_do(i, nLayer[i]);
        free(pfInitWeight);
    }

    cudnnCreateTensorDescriptor(&srcTensorDesc);
    cudnnCreateTensorDescriptor(&dstTensorDesc);
    cudnnCreateTensorDescriptor(&biasTensorDesc);
    cudnnCreateFilterDescriptor(&weightDesc);
    cudnnCreateConvolutionDescriptor(&convDesc);

    IN_WIDTH = 28; IN_HEIGHT = 28; IN_CHANNEL = 512; 
    OUT_WIDTH = 28; OUT_HEIGHT = 28; OUT_CHANNEL = 256; 
	cudnn_convolutional_setup();
	fw_algo_512 = getForwardAlgorithm();

    IN_WIDTH = 14; IN_HEIGHT = 14; IN_CHANNEL = 1024;
    OUT_WIDTH = 14; OUT_HEIGHT = 14; OUT_CHANNEL = 256;
	cudnn_convolutional_setup();
	fw_algo_1024 = getForwardAlgorithm();

}


/////////////////////////////////////
// go code
/////////////////////////////////////

static size_t get_workspace_size() {
    size_t most = 0;
    size_t s = 0;
    cudnnGetConvolutionForwardWorkspaceSize(cudnn_handle(), srcTensorDesc,
                                            weightDesc, convDesc, dstTensorDesc,
                                            fw_algo, &s);
    if (s > most)
        most = s;
    return most;
}


static int global_iter = 0;
static int iter = 0;
const float one = 1;
TASK_GO
{
    // TODO: task main code
    int layer_idx = 1;
    int i;
    int nInSize, nOutSize, nMaxInSize, nMaxOutSize;

	iter = iter % EXECUTION_COUNT;

    layer_idx = nLayer[iter];
    
    if(layer_idx == 35){
        IN_WIDTH = 28; IN_HEIGHT = 28; IN_CHANNEL = 512; 
        OUT_WIDTH = 28; OUT_HEIGHT = 28; OUT_CHANNEL = 256;
		fw_algo = fw_algo_512;
    }
    else{
        IN_WIDTH = 14; IN_HEIGHT = 14; IN_CHANNEL = 1024;
        OUT_WIDTH = 14; OUT_HEIGHT = 14; OUT_CHANNEL = 256;
		fw_algo = fw_algo_1024;
    }
    nMaxInSize = 28 * 28 * 512;
    nMaxOutSize = 28 * 28 * 256;
    nInSize = IN_WIDTH * IN_HEIGHT * IN_CHANNEL;
    nOutSize = OUT_WIDTH * OUT_HEIGHT * OUT_CHANNEL;

    cudnn_convolutional_setup();

    workspace_size = get_workspace_size();
    if(workspace_size > workspace_maxsize){
        if(pfWorkspace != NULL) cudaFree(pfWorkspace);
        pfWorkspace = cuda_make_array((float *)NULL, workspace_size);
        workspace_maxsize = workspace_size;
	}

    check_error(cudaMemsetAsync(pfOutputGPU, 0, sizeof(float) * BATCH * nOutSize, stream));    

    MQ_RECEIVE(port_input, (unsigned char*)pfInputGPU, sizeof(float) * nMaxInSize * BATCH);

    cudnnConvolutionForward(cudnn_handle(), &one, srcTensorDesc, pfInputGPU,
                            weightDesc, pfWeightGPU[iter], convDesc, fw_algo,
                            pfWorkspace, workspace_size, &one, dstTensorDesc,
                            pfOutputGPU);
    cudnnAddTensor(cudnn_handle(), &one, biasTensorDesc, pfBiasGPU[iter], &one, dstTensorDesc, pfOutputGPU);

    leaky_relu_gpu(pfOutputGPU, nOutSize * BATCH);

    if(global_iter < 0){ 
        pfData = (float *)malloc(sizeof(float) * nOutSize * BATCH);
        cudaMemcpy(pfData, pfOutputGPU, sizeof(float) * nOutSize * BATCH, cudaMemcpyDeviceToHost);
        int j, k;
        printf("##############################################\n");
        printf("# thirdblock_third_conv) layer_idx : %d\n",layer_idx);
        if(global_iter == 0) printf("# c35\n");
        if(global_iter == 1) printf("# c38\n");
        for(i = 0; i < OUT_WIDTH; i++){
            if(i == 0){ 
                for(j = 0; j < OUT_HEIGHT; j++){
                    if(j == 0){ 
                        for(k = 0; k < OUT_CHANNEL; k++){
                            printf("%.5f ",pfData[i * OUT_HEIGHT * OUT_CHANNEL + j * OUT_CHANNEL + k]);
                            if(k % 16 == 15) printf("\n");
                        }   
                    }   
                }   
            }   
        }   
        free(pfData);
        printf("##############################################\n");
    } 

    layer_idx++;
    iter++; //change to loop API
	global_iter++;
    MQ_SEND(port_output, (unsigned char*)pfOutputGPU, sizeof(float) * nMaxOutSize * BATCH);
}



/////////////////////////////////////
// wrapup code
/////////////////////////////////////

TASK_WRAPUP
{
    // TODO: task wrapup code
    int i;
    for(i = 0; i < EXECUTION_COUNT; i++){
        cudaFree(pfWeightGPU[i]);
        cudaFree(pfBiasGPU[i]);
    }
    cudaFree(pfInputGPU);
    cudaFree(pfOutputGPU);
}

TASK_CODE_END
